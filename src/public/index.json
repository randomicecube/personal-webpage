[{"content":"Description Galápagos has been developed as part of the work toward a research paper submitted to the IEEE Transactions on Dependable and Secure Computing (TDSC) journal. The paper was authored by the ASSERT team at KTH Royal Institute of Technology, Sweden, and is currently under review. In particular, the authors are Javier Ron, Javier Cabrera-Arteaga, Martin Monperrus, and Benoit Baudry.\nThe work explores the integration of N-version programming with automatic programming language translation, utilizing large language models (LLMs). It also makes use of a formal equivalence checker to validate the correctness of the generated program variants. The system only accepts pure functions as input. It is available as an open-source project on GitHub.\nFor our evaluation, we used open-source C-based projects from GitHub, such as FFmpeg and openssl, programs and tools which are ubiquitous in the software industry.\nContributions As for my contributions within the project, my work was focused on refactoring the tool, based on an initial version which was under development when I joined the project. I was able to refactor core sections of the codebase, including testing and use case handling, into a more modular, reusable, and maintainable structure. I also helped in documenting the tool, as well as choosing the projects and functions for evaluation.\n","permalink":"//localhost:1313/projects/galapagos/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eGalápagos has been developed as part of the work toward a \u003ca href=\"https://arxiv.org/pdf/2408.09536\" target=\"_blank\" rel=\"noopener\"\u003eresearch paper\u003c/a\u003e\nsubmitted to the IEEE Transactions on Dependable and Secure Computing (TDSC) journal.\nThe paper was authored by the ASSERT team at KTH Royal Institute of Technology, Sweden,\nand is currently under review.\nIn particular, the authors are \u003ca href=\"https://github.com/javierron\" target=\"_blank\" rel=\"noopener\"\u003eJavier Ron\u003c/a\u003e,\n\u003ca href=\"Jacarte\"\u003eJavier Cabrera-Arteaga\u003c/a\u003e,\n\u003ca href=\"https://github.com/monperrus\" target=\"_blank\" rel=\"noopener\"\u003eMartin Monperrus\u003c/a\u003e,\nand \u003ca href=\"https://github.com/bbaudry\" target=\"_blank\" rel=\"noopener\"\u003eBenoit Baudry\u003c/a\u003e.\u003c/p\u003e","title":"Galápagos: Automated N-Version Programming with LLMs"},{"content":"Description Our group, in collaboration with KTH and SEB, aimed to address challenges such as triplet extraction reliability from unstructured data for Knowledge Graphs (KGs) by employing pre-trained large language models (LLMs). In particular, we explored two distinct yet complementary methodologies to improve Knowledge Graph construction in the financial domain: question-answer (Q\u0026amp;A) generation, as well as a consensus strategy for triplet extraction, both being subject to automated evaluation to test their performance.\nThe Q\u0026amp;A generation experiment demonstrated the efficacy of Vector RAG over Graph RAG, with significant improvements in retrieval accuracy, particularly when paired with advanced embedding strategies. This approach leveraged language models for both question generation and validation, showcasing the robustness of retrieval-augmented generation in extracting actionable insights from KGs.\nIn contrast, the consensus strategy (evaluated as displayed above) introduced a unique pipeline that integrated outputs from multiple language models using veto and majority policies, combined with various matching techniques (exact, fuzzy, and embeddings with cosine similarity). Notably, the veto policy with embedding-based comparison emerged as the most effective configura- tion, achieving a balanced trade-off between diversity and precision in triplet extraction. While the consensus approach did not universally enhance accuracy, it provided valuable insights into refining model outputs for complex data scenarios.\nTogether, these findings highlight the complementary nature of Q\u0026amp;A-driven evaluation and consensus-based refinement. Vector RAG excelled in question-answering tasks, while embedding-based veto policies provided nuanced improvements in triplet extraction. This dual approach underscores the potential of combining advanced retrieval and consensus strategies to address the challenges of financial KG construction.\nOverall, we believe that the project successfully showcased the potential of integrating LLMs for financial KG construction.\nContributions My primary contributions to the project included the integration of language models into the backend, as well as researching for suitable ones. Furthermore, I was also responsible for the research of suitable evaluation metrics, and helping in conceptualizing and implementing the consensus strategy for triplet extraction. Finally, I was also assigned the task of team manager, which involved everything from more bureaucratic tasks, such as contacting SEB and KTH whenever necessary, to keeping a diary of the team\u0026rsquo;s progress (individually and collectively) along the weeks, and ensuring that everyone was on track with their tasks.\n","permalink":"//localhost:1313/projects/seb-kg-llm/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eOur group, in collaboration with KTH and SEB, aimed to address challenges such as\ntriplet extraction reliability from unstructured data for Knowledge Graphs (KGs)\nby employing pre-trained large language models (LLMs).\nIn particular, we explored two distinct yet complementary methodologies to improve Knowledge Graph\nconstruction in the financial domain: question-answer (Q\u0026amp;A) generation, as well as a consensus\nstrategy for triplet extraction, both being subject to automated evaluation to test their performance.\u003c/p\u003e","title":"Knowledge Graph construction with Language Models"},{"content":"Description Worked alongside Diogo Correia, as well as a huge group of contributors on the creation and maintenance of ResumosLEIC, a website which compiles tools and course notes from the Computer Science and Engineering Bachelor\u0026rsquo;s degree at my university.\nContributions I worked as the main contributor for the class notes of courses such as Algorithms\u0026rsquo; Synthesis and Analysis and Introduction to Algorithms and Data Structures, among a plethora of others, as well as updating the content of courses across years, to ensure that the information was always up-to-date with the latest syllabus.\nFurthermore, was, alongside Diogo Correia, responsible for the maintenance and acceptance of new class notes, being a reviewer of hundreds of pull requests from contributors, both new and old, to ensure that the quality of the content was always as best as possible.\n","permalink":"//localhost:1313/projects/resumos-leic/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/resumos-leic/leic.png#center\" alt=\"ResumosLEIC\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eWorked alongside \u003ca href=\"https://diogotc.com\" target=\"_blank\" rel=\"noopener\"\u003eDiogo Correia\u003c/a\u003e, as well as a huge \u003ca href=\"https://github.com/leic-pt/resumos-leic/graphs/contributors\" target=\"_blank\" rel=\"noopener\"\u003egroup of contributors\u003c/a\u003e\non the creation and maintenance of \u003ca href=\"https://leic.pt\" target=\"_blank\" rel=\"noopener\"\u003eResumosLEIC\u003c/a\u003e, a website which\ncompiles tools and course notes from the Computer Science and Engineering\nBachelor\u0026rsquo;s degree at my university.\u003c/p\u003e\n\u003ch2 id=\"contributions\"\u003eContributions\u003c/h2\u003e\n\u003cp\u003eI worked as the main contributor for the class notes\nof courses such as \u003ca href=\"https://leic.pt/asa\" target=\"_blank\" rel=\"noopener\"\u003eAlgorithms\u0026rsquo; Synthesis and Analysis\u003c/a\u003e\nand \u003ca href=\"https://leic.pt/iaed\" target=\"_blank\" rel=\"noopener\"\u003eIntroduction to Algorithms and Data Structures\u003c/a\u003e,\namong a plethora of others, as well as updating the content of courses across years,\nto ensure that the information was always up-to-date with the latest syllabus.\u003c/p\u003e","title":"ResumosLEIC"},{"content":"Description We recently submitted a research paper to the IEEE Transactions on Dependable and Secure Computing (TDSC) journal, which explores the integration of N-version programming with automatic programming language translation, utilizing large language models (LLMs). Additionally, I undertook a complete refactoring of a tool, developed in both Python and C++, designed to aid in the development and testing of LLM-driven automatic code translation, for the aforementioned paper.\nCurrently, my focus is on developing an automated feedback system aimed at addressing software supply chain errors commonly made by students, using dirty-waters as its driver. Toward this, I have had to focus on this both from a research assistant and as a Master\u0026rsquo;s thesis student perspective:\nFrom the former, I have been responsible for implementing Maven support for dirty-waters, as well as bug fixing, refactoring, documenting, and implementing caching features \u0026ndash; the latter of which has been particularly challenging but rewarding, with the system now being able to run in around 5-10s, down from 10-15 minutes for the Spoon project, as an example. From the latter, I have been responsible for the creation of a CI action for dirty-waters, in order to be able to use the tool in an easier manner, both in student and developer environments. I will then proceed to evaluate the tool in real-world scenarios, by being employed both in open-source projects as well as in courses at KTH. ","permalink":"//localhost:1313/experience/chains/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eWe recently submitted a research paper to the IEEE Transactions on Dependable and Secure Computing (TDSC) journal,\nwhich explores the integration of N-version programming with automatic programming language translation,\nutilizing large language models (LLMs).\nAdditionally, I undertook a complete refactoring of a tool, developed in both Python and C++, designed\nto aid in the development and testing of LLM-driven automatic code translation, for the aforementioned paper.\u003c/p\u003e\n\u003cp\u003eCurrently, my focus is on developing an automated feedback system aimed at addressing software supply chain errors\ncommonly made by students, using \u003ca href=\"https://github.com/chains-project/dirty-waters\" target=\"_blank\" rel=\"noopener\"\u003edirty-waters\u003c/a\u003e as its driver.\nToward this, I have had to focus on this both from a research assistant and as a Master\u0026rsquo;s thesis student\nperspective:\u003c/p\u003e","title":"Research Assistant"},{"content":"Description Played a role in developing, testing, and maintaining several components of my university’s learning management system, FenixEdu, which is built using Java and the Spring framework. My major contribution was the development, alongside a team of several members, ranging from senior colleagues to other student developers, of a new version of the university\u0026rsquo;s public API, featuring dozens of endpoints written in Java (Spring) and thoroughly documented using YAML (OpenAPI). To enhance accessibility, we created a user-centered display of the public API by setting up a custom website with NodeJS. We also optimized the deployment process by implementing a custom GitLab CI/CD pipeline with four stages, which significantly reduced deployment time and improved image-building reliability through automated version releases. Additionally, we improved the responsiveness and reliability of the university’s React Native mobile app by identifying and fixing bugs, alongside testing.\n","permalink":"//localhost:1313/experience/dasi/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003ePlayed a role in developing, testing, and maintaining several components of my university’s learning management system, FenixEdu, which is built using Java and the Spring framework. My major contribution was the development, alongside a team of several members, ranging from senior colleagues to other student developers, of a new version of the university\u0026rsquo;s public API, featuring dozens of endpoints written in Java (Spring) and thoroughly documented using YAML (OpenAPI). To enhance accessibility, we created a user-centered display of the public API by setting up a custom website with NodeJS. We also optimized the deployment process by implementing a custom GitLab CI/CD pipeline with four stages, which significantly reduced deployment time and improved image-building reliability through automated version releases. Additionally, we improved the responsiveness and reliability of the university’s React Native mobile app by identifying and fixing bugs, alongside testing.\u003c/p\u003e","title":"Full-Stack Developer"}]